const fs = require('fs');
const path = require('path');
const ffmpeg = require('fluent-ffmpeg');
const ffmpegPath = require('ffmpeg-static');
const wav = require('node-wav');

if (ffmpegPath) {
  ffmpeg.setFfmpegPath(ffmpegPath);
} else {
  console.warn('‚ö†Ô∏è ffmpeg-static could not determine a binary path. Ensure FFmpeg is installed and available in PATH.');
}

class SpeechService {
  constructor() {
    this.pipelinePromise = null;
    this.modelName = process.env.WHISPER_MODEL || 'Xenova/whisper-small';
    this.modelQuantized = process.env.WHISPER_QUANTIZED !== 'false';
  }

  /**
   * Lazy load Whisper model
   */
  async getPipeline() {
    if (!this.pipelinePromise) {
      this.pipelinePromise = import('@xenova/transformers').then(async ({ pipeline }) => {
        console.log(`üîÅ Loading Whisper model: ${this.modelName} (quantized=${this.modelQuantized})`);
        const start = Date.now();
        const whisperPipeline = await pipeline('automatic-speech-recognition', this.modelName, {
          quantized: this.modelQuantized,
        });
        const duration = ((Date.now() - start) / 1000).toFixed(2);
        console.log(`‚úÖ Whisper model loaded in ${duration}s`);
        return whisperPipeline;
      }).catch(error => {
        console.error('‚ùå Failed to load Whisper model:', error);
        this.pipelinePromise = null;
        throw error;
      });
    }
    return this.pipelinePromise;
  }

  /**
   * Convert input audio to 16kHz mono WAV (required by Whisper)
   * @param {string} inputPath
   * @returns {Promise<string>} output WAV path
   */
  async convertToWav(inputPath) {
    const outputPath = `${inputPath}-converted.wav`;

    return new Promise((resolve, reject) => {
      ffmpeg(inputPath)
        .outputOptions([
          '-ac 1', // mono
          '-ar 16000', // 16kHz
          '-f wav',
        ])
        .on('end', () => {
          resolve(outputPath);
        })
        .on('error', (error) => {
          reject(new Error(`FFmpeg conversion failed: ${error.message}`));
        })
        .save(outputPath);
    });
  }

  /**
   * Chuy·ªÉn ƒë·ªïi audio th√†nh text b·∫±ng Whisper
   * @param {string} audioFilePath - ƒê∆∞·ªùng d·∫´n ƒë·∫øn file audio
   * @param {object} options - T√πy ch·ªçn: languageCode (g·ª£i √Ω, Whisper t·ª± detect)
   * @returns {Promise<object>} {text, languageCode}
   */
  async transcribeAudio(audioFilePath, options = {}) {
    const normalizeLanguageCode = (code) => {
      if (!code || typeof code !== 'string') return undefined;
      // L·∫•y ph·∫ßn ng√¥n ng·ªØ ch√≠nh (v√≠ d·ª• vi-VN -> vi, en-US -> en)
      const base = code.split(/[-_]/)[0].toLowerCase();
      // Danh s√°ch chu·∫©n ƒë∆°n gi·∫£n cho c√°c ng√¥n ng·ªØ hay d√πng
      const supported = new Set([
        'vi','en','fr','de','es','it','pt','ru','ja','ko','zh','ar','hi','id','th','tr'
      ]);
      return supported.has(base) ? base : undefined;
    };

    let wavPath = null;
    try {
      const pipeline = await this.getPipeline();

      // Whisper c·∫ßn WAV 16kHz mono -> convert
      wavPath = await this.convertToWav(audioFilePath);

      console.log(`üîç Transcribing audio with Whisper...`);
      const start = Date.now();

      // ƒê·ªçc d·ªØ li·ªáu WAV v√†o b·ªô nh·ªõ v√† gi·∫£i m√£ th√†nh Float32Array
      const wavBuffer = await fs.promises.readFile(wavPath);
      const decoded = wav.decode(wavBuffer);
      if (!decoded || !decoded.channelData || decoded.channelData.length === 0) {
        throw new Error('Decoded audio is empty');
      }
      const audioInput = decoded.channelData[0]; // Float32Array k√™nh ƒë∆°n

      const result = await pipeline(audioInput, {
        chunk_length_s: 30,
        stride_length_s: 5,
        language: normalizeLanguageCode(options.languageCode),
        task: 'transcribe',
        return_timestamps: false,
        sampling_rate: decoded.sampleRate,
      });

      const duration = ((Date.now() - start) / 1000).toFixed(2);
      console.log(`üìù Whisper result: "${result?.text?.trim() || ''}"`);
      console.log(`‚è±Ô∏è Whisper processing time: ${duration}s`);

      // X√≥a file convert t·∫°m
      if (fs.existsSync(wavPath)) {
        fs.unlinkSync(wavPath);
      }

      return {
        text: (result?.text || '').trim(),
        confidence: null,
        languageCode: result?.language || options.languageCode || 'unknown',
      };
    } catch (error) {
      if (wavPath && fs.existsSync(wavPath)) {
        fs.unlinkSync(wavPath);
      }
      console.error('‚ùå Whisper transcription error:', error);
      throw new Error(`Whisper transcription failed: ${error.message}`);
    }
  }

  /**
   * Parse text th√†nh command cho h·ªá th·ªëng IoT
   * @param {string} text - Text t·ª´ speech-to-text
   * @returns {object} Command object
   */
  parseCommand(text) {
    if (!text || typeof text !== 'string') {
      return {
        action: 'unknown',
        amount: null,
        unit: null,
        rawText: text,
      };
    }

    const lowerText = text.toLowerCase().trim();
    const command = {
      action: 'unknown',
      amount: null,
      unit: 'gram',
      rawText: text,
      confidence: 'low',
    };

    // T√¨m s·ªë l∆∞·ª£ng (gram, g, kg)
    const amountPatterns = [
      /(\d+)\s*(?:gram|g|gr)/i,
      /(\d+)\s*(?:kilogram|kg)/i,
      /cho\s+(\d+)/i,
      /(\d+)/,
    ];

    for (const pattern of amountPatterns) {
      const match = lowerText.match(pattern);
      if (match) {
        let amount = parseInt(match[1]);
        
        // Chuy·ªÉn kg sang gram
        if (lowerText.includes('kg') || lowerText.includes('kilogram')) {
          amount = amount * 1000;
          command.unit = 'gram';
        }
        
        command.amount = amount;
        break;
      }
    }

    // X√°c ƒë·ªãnh action
    const feedKeywords = [
      'cho ƒÉn',
      'cho th√∫ c∆∞ng ƒÉn',
      'cho pet ƒÉn',
      'feed',
      'feeding',
      'cho th·ª©c ƒÉn',
      'ƒë·ªï th·ª©c ƒÉn',
    ];

    const stopKeywords = [
      'd·ª´ng',
      'stop',
      'ng·ª´ng',
      't·∫Øt',
      'cancel',
    ];

    const statusKeywords = [
      'ki·ªÉm tra',
      'xem',
      'check',
      'status',
      't√¨nh tr·∫°ng',
      'l∆∞·ª£ng th·ª©c ƒÉn',
    ];

    if (feedKeywords.some(keyword => lowerText.includes(keyword))) {
      command.action = 'feed';
      command.confidence = 'high';
    } else if (stopKeywords.some(keyword => lowerText.includes(keyword))) {
      command.action = 'stop';
      command.confidence = 'high';
    } else if (statusKeywords.some(keyword => lowerText.includes(keyword))) {
      command.action = 'status';
      command.confidence = 'high';
    } else if (command.amount !== null) {
      // N·∫øu c√≥ s·ªë l∆∞·ª£ng nh∆∞ng kh√¥ng r√µ action, m·∫∑c ƒë·ªãnh l√† feed
      command.action = 'feed';
      command.confidence = 'medium';
    }

    // X·ª≠ l√Ω c√°c l·ªánh ƒë·∫∑c bi·ªát
    if (lowerText.includes('m·∫∑c ƒë·ªãnh') || lowerText.includes('default')) {
      command.action = 'feed';
      command.amount = null; // D√πng l∆∞·ª£ng m·∫∑c ƒë·ªãnh
      command.confidence = 'high';
    }

    return command;
  }

  /**
   * Ki·ªÉm tra k·∫øt n·ªëi v·ªõi Google Cloud
   */
  async testConnection() {
    try {
      await this.getPipeline();
      return {
        connected: true,
        message: `Whisper model ${this.modelName} is ready`,
      };
    } catch (error) {
      return {
        connected: false,
        error: error.message,
      };
    }
  }
}

// Export singleton instance
module.exports = new SpeechService();

